{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bfad2d2",
   "metadata": {},
   "source": [
    "# MLP Regression for 2(a) and 2(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a18c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define sigmoid and its derivative\n",
    "def sigmoid(h):\n",
    "    h = np.clip(h, -50, 50)\n",
    "    return 1.0 / (1.0 + np.exp(-h))\n",
    "\n",
    "def sigmoid_prime(h):\n",
    "    s = sigmoid(h)\n",
    "    return s * (1.0 - s)\n",
    "\n",
    "# Define the MLP class (one hidden layer, sigmoid activation)\n",
    "class OneHiddenMLP:\n",
    "    def __init__(self, n_inputs, n_hidden, alpha0=0.5, eta=0.9, eps=1e-3, max_epochs=2000, rng=None):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.alpha0 = alpha0\n",
    "        self.eta = eta\n",
    "        self.eps = eps\n",
    "        self.max_epochs = max_epochs\n",
    "        self.rng = rng if rng is not None else np.random.default_rng()\n",
    "\n",
    "        # We include bias as an extra \"input\" with fixed value -1\n",
    "        self.W0 = self.rng.uniform(-0.1, 0.1, size=(n_hidden, n_inputs + 1))  # Hidden layer weights\n",
    "        self.W1 = self.rng.uniform(-0.1, 0.1, size=(n_hidden + 1,))           # Output layer weights\n",
    "\n",
    "    def _forward(self, X):\n",
    "        N = X.shape[0]\n",
    "        Xb = np.hstack([ -np.ones((N, 1)), X ])   # Add bias to inputs\n",
    "        h = Xb @ self.W0.T                          # Hidden layer\n",
    "        H = sigmoid(h)                              # Sigmoid activations\n",
    "\n",
    "        Hb = np.hstack([ -np.ones((N, 1)), H ])     # Add bias to hidden layer\n",
    "        o_in = Hb @ self.W1                         # Output layer\n",
    "        O = o_in                                    # Linear output (g(h) = h)\n",
    "        return Xb, h, H, Hb, o_in, O\n",
    "\n",
    "    def predict(self, X):\n",
    "        _, _, _, _, _, O = self._forward(X)\n",
    "        return O\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        N = X.shape[0]\n",
    "        alpha = self.alpha0\n",
    "        epoch = 0\n",
    "\n",
    "        while alpha > self.eps and epoch < self.max_epochs:\n",
    "            Xb, h, H, Hb, o_in, O = self._forward(X)\n",
    "\n",
    "            e = y - O                             # Errors\n",
    "            delta_out = (O - y) / N               # Derivative for output\n",
    "            grad_W1 = Hb.T @ delta_out            # Gradient for W1\n",
    "\n",
    "            tmp = delta_out[:, None] * self.W1[1:][None, :]\n",
    "            delta_hidden = sigmoid_prime(h) * tmp  # Gradient for hidden layer\n",
    "            grad_W0 = delta_hidden.T @ Xb         # Gradient for W0\n",
    "\n",
    "            # Update weights\n",
    "            self.W1 -= alpha * grad_W1\n",
    "            self.W0 -= alpha * grad_W0\n",
    "\n",
    "            # Decay learning rate\n",
    "            alpha *= self.eta\n",
    "            epoch += 1\n",
    "        return\n",
    "\n",
    "# Training function that selects the best number of hidden units (J)\n",
    "def train_mlp_and_select_hidden(X_train, y_train, X_test, y_test,\n",
    "                                n_inputs,\n",
    "                                J0=3, alpha0=0.5, eta=0.9, eps=1e-3,\n",
    "                                max_hidden=20):\n",
    "    rng = np.random.default_rng(123)\n",
    "\n",
    "    J = J0\n",
    "    best_J = J\n",
    "    best_test_mse = np.inf\n",
    "    best_model = None\n",
    "    results = []\n",
    "\n",
    "    while J <= max_hidden:\n",
    "        print(f\"\n",
    "Training MLP with J = {J} hidden units...\")\n",
    "        model = OneHiddenMLP(n_inputs, J, alpha0=alpha0, eta=eta, eps=eps, max_epochs=2000, rng=rng)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_hat_s = model.predict(X_train)\n",
    "        y_test_hat_s  = model.predict(X_test)\n",
    "\n",
    "        train_sse = np.sum((y_train - y_train_hat_s) ** 2)\n",
    "        test_sse  = np.sum((y_test  - y_test_hat_s) ** 2)\n",
    "        test_mse  = test_sse / len(y_test)\n",
    "        test_var  = np.var((y_test - y_test_hat_s) ** 2, ddof=1)\n",
    "\n",
    "        print(f\"J = {J}: train SSE = {train_sse:.4f}, test MSE = {test_mse:.4f}\")\n",
    "\n",
    "        results.append((J, train_sse, test_mse, test_var, model))\n",
    "\n",
    "        if test_mse < best_test_mse:\n",
    "            best_test_mse = test_mse\n",
    "            best_J = J\n",
    "            best_model = model\n",
    "            J += 1  # try more units\n",
    "        else:\n",
    "            # Test error did not improve -> stop\n",
    "            break\n",
    "\n",
    "    print(f\"\n",
    "Best number of hidden units (approx): J = {best_J}\")\n",
    "    return best_J, best_model, results\n",
    "\n",
    "# Now run the function with our dataset\n",
    "best_J, best_model, results = train_mlp_and_select_hidden(\n",
    "    X_train, y_train_s, X_test, y_test_s,\n",
    "    n_inputs=1,  # One input: x\n",
    "    J0=3, alpha0=0.5, eta=0.9, eps=1e-3, max_hidden=15\n",
    ")\n",
    "\n",
    "# Plotting the final result for 2(a)\n",
    "xs = np.linspace(x_all.min(), x_all.max(), 200)\n",
    "zs = (xs - x_mean) / x_std  # Standardize the input values\n",
    "Xs = zs.reshape(-1, 1)\n",
    "ys_hat_s = best_model.predict(Xs)\n",
    "ys_hat = y_mean + y_std * ys_hat_s\n",
    "\n",
    "# Plot the results\n",
    "plt.figure()\n",
    "plt.scatter(x_train, y_train, label=\"Train\", alpha=0.6)\n",
    "plt.scatter(x_test,  y_test,  label=\"Test\",  alpha=0.6)\n",
    "plt.plot(xs, ys_hat, label=f\"MLP 2(a), J={best_J}\", linewidth=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"2(a) MLP regression (input = standardized x)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now run the function with our dataset for 2(b) MLP model (x and x^2 as input)\n",
    "best_J, best_model, results = train_mlp_and_select_hidden(\n",
    "    X_train, y_train_s, X_test, y_test_s,\n",
    "    n_inputs=2,  # Two inputs: x and x^2\n",
    "    J0=3, alpha0=0.5, eta=0.9, eps=1e-3, max_hidden=15\n",
    ")\n",
    "\n",
    "# Output the results for 2(b)\n",
    "best_J, results[-1]  # The best number of hidden units and the last result (best performance)\n",
    "\n",
    "# Plotting the final result for 2(b)\n",
    "xs = np.linspace(x_all.min(), x_all.max(), 200)\n",
    "zs = (xs - x_mean) / x_std  # Standardize the input values\n",
    "Xs = np.column_stack([zs, zs**2])\n",
    "ys_hat_s = best_model.predict(Xs)\n",
    "ys_hat = y_mean + y_std * ys_hat_s\n",
    "\n",
    "# Plot the results for 2(b)\n",
    "plt.figure()\n",
    "plt.scatter(x_train, y_train, label=\"Train\", alpha=0.6)\n",
    "plt.scatter(x_test,  y_test,  label=\"Test\",  alpha=0.6)\n",
    "plt.plot(xs, ys_hat, label=f\"MLP 2(b), J={best_J}\", linewidth=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"2(b) MLP regression (inputs = standardized x, x^2)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
